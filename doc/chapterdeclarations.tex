% $Revision$
% $Id$


\chapter{Declarations and Bindings} \label{declarations}
\index{package!declaration} \index{declaration} \index{declaration!package}

In this chapter, we describe the syntax and informal semantics of \frege{} \emph{declarations}.

Each source file contains exactly one package declaration.

\begin{flushleft}
\rul{program} \term{package} \nont{packagename} \term{where} \bracea{} \nont{body} \bracez{}
  \alt \term{package} \nont{packagename} \sym{;} \nont{body} \\
\rul{packagename} \nont{qconid}
  \alt \nont{qvarid} \sym{.} \nont{packagename}
  \alt \nont{qconid} \sym{.} \nont{packagename}\\
\rul{body} \nont{topdecl}
  \alt \nont{topdecl} \sym{;} \nont{body}\\
\rul{topdecl} \nont{fixity} \gcom{see \autoref{fixity}}
  \alt \nont{pkgimport}        \gcom{package import}
  \alt \nont{typedcl}          \gcom{type alias declaration}
  \alt \nont{datadcl}          \gcom{data type declaration}
  \alt \nont{classdcl}         \gcom{type class declaration}
  \alt \nont{instdcl}          \gcom{instance declaration}
  \alt \nont{derivedcl}        \gcom{derived instance declaration}
  \alt \nont{decl}\\
\rul{decl} \nont{annotation}    \gcom{type annotation}
  \alt  \nont{binding}          \gcom{function or pattern binding}
  \alt  \nont{nativefun}        \gcom{native function}\\
\rul{decls} \liste{\nont{decl}}{\sym{;}}\\

%\rul{pkgimport} \term{import} \nont{packagename} \opt{\nont{importlist}} \opt{\sym{as} \nont{namespace}}\\
%\rul{importlist} \sym{(} \some{importitem} \sym{)} \\
%\rul{importitem} \nont{qvarid} \oder{} \nont{qconid} \oder{} \nont{lexop} \oder{} \nont{qunop}\\
%\rul{namespace} \nont{conid}
\end{flushleft}

The \emph{packagename} is a sequence of one or more identifiers where the last identifier starts with an uppercase letter.
The package declaration opens a namespace with a name that equals the last component of the package name.
Each top level item can be accessed with a qualified name that uses the namespace as qualifier, thus in\exq{
package my.fine.pack.with.a.long.name.X where\\
foo = 42\\
bar = let foo = 24 in foo + X.foo
}
\texttt{bar} evaluates to 66.

\index{declaration!top level}

The \emph{body} is a list of top level declarations. The first form of the package declaration gives an opportunity to use \hyperref[layout]{layout} for the top level by omitting the braces around the \emph{body}. The second form \exq{
\textbf{package} \emph{packagename}\sym{;} \emph{body}}
looks more \java{}-ish and is equivalent to \exq{
\textbf{package} \emph{packagename} \textbf{where} \bracea{} \emph{body} \bracez{}
}
The layout rule cannot be used at the top level in this case.

It is immaterial in which order the declarations are given.

The declarations in the syntactic category \nont{topdecl} are only allowed at the top level of a \frege{} package, whereas \nont{decl}s can be used either at the top level or in the scope of a data type, class or instance.

For exposition, we divide the declarations into three groups: user defined data types, consisting of type and data declarations; type classes and overloading, consisting of class and instance declarations; and nested declarations, consisting of value bindings and type signatures.

\section{Overview of Types and Classes}
\frege{} uses an polymorphic type system based on the traditional Hindley-Milner type system \cite{ptifart} to provide a static type semantics. The type system supports \emph{type classes} or just \emph{classes} that provide a structured way to introduce \emph{overloaded} functions.

A \texttt{class} declaration (\autoref{classdcl}) introduces a new \emph{type class} and the overloaded operations that must be supported by any type that is an instance of that class. An \texttt{instance} declaration (\autoref{instdcl}) declares that a type is an \emph{instance} of a class and includes the definitions of the overloaded operations - called \emph{class methods} - instantiated on the named type. \index{class} \index{instance}

\java{} programmers are familiar with the concept of \emph{interfaces} which serve a similar purpose like \emph{type classes}, but there is a fundamental difference: A \frege{} type class is not a type in its own right like a \java{} interface. Instances of type classes are types, instances of \java{} interfaces are objects.

\subsection{Syntax of Types} \index{type} \index{type!syntax} \label{typesyntax}

The syntax for \frege{} type expressions is given below. Just as data values are built using data constructors, type values are built from \emph{type constructors}. As with data constructors the names of type constructors start with uppercase letters.

The names of type constructors and data constructors will never be confused (by the compiler, that is) as the former only appear in types and the latter in expressions.

\begin{flushleft}
\rul{type} \nont{tyapp} \sym{\arrow{}} \nont{type} \gcom{function type}
  \alt \nont{tyapp}\\
\rul{tyapp} \nont{tyapp} \nont{simpletype}  \gcom{type application}
  \alt \nont{simpletype}\\
\rul{simpletype} \nont{tyvar} \gcom{type variable}
  \alt \nont{tycon}           \gcom{type constructor}
  \alt \sym{(} \nont{type} \sym{)}
  \alt \sym{(} \nont{type} \sym{,} \liste{\nont{type}}{\sym{,}} \sym{)} \gcom{tuple type}
  \alt \sym{\bracka{}} \nont{type} \sym{\brackz{}}  \gcom{list type}
  \alt \sym{\bracea{}} \liste{\nont{fieldtyp}}{\sym{,}} \sym{\bracez{}} \gcom{\hyperref[recordtypes]{record type}}\\
\rul{tyvar} \nont{varid}
  \alt \nont{classname} \sym{:} \nont{tyvar}    \gcom{type variable with class constraint}
  \alt \sym{\bracea{}} \nont{tyvar} \sym{$|$} \liste{\nont{fieldtyp}}{\sym{,}} \sym{\bracez{}} \gcom{type variable with record constraint}\\
\rul{tycon} \nont{qconid}
  \alt \sym{\bracka\brackz}  \gcom{list type constructor}
  \alt \sym{()}              \gcom{unit type constructor}
  \alt \sym{(}\more{\sym{,}}\sym{)} \gcom{tuple type constructors}\\
\rul{fieldtyp} \nont{varid} \sym{::} \nont{type} \gcom{field name with type}
  \alt \nont{varid} \gcom{field name with polymorphic type}\\
\rul{classname} \nont{qconid}
\end{flushleft}

The main forms of type expressions are as follows:

\begin{enumerate}
\item Type variables written as identifiers beginning with a lowercase letter. Variables can be constrained in two forms: \label{constraint}
\begin{description}
\item[\emph{class constraints}] makes sure that the type variable can be instantiated only with a type that is an instance of the given class.
\item[\emph{record constraint}] makes sure that the type variable can be instantiated only with a record type that has the listed fields with the given types.
\end{description}
\item Type constructors. Type constructors are written as identifiers beginning with an uppercase letter. The identifiers may be qualified. Type constructors denote either a user defined data type or a type alias.

Special syntax is provided for the type constructors of the list type, the trivial type and the tuple types.

Type constructors can be constants like \texttt{Int} that denotes the integer type or they can be polymorphic like the list type.
\item Type application. Polymorphic type constructors (or type variables that stand for them) must be applied to type parameters to denote a complete type. For example, the list type constructor requires a type for the elements of the list.
\item A parenthesized type ($t$) is identical to the type $t$, but the parenthesis may be required for syntactical reasons.
\end{enumerate}

Special syntax is provided to allow certain type expressions to be written in a more traditional style:

\begin{enumerate}
\item A \emph{function type} has the form $t_1$ \arrow{} $t_2$. Function arrows associate to the right, thus $t_1$ \arrow{} ($t_2$ \arrow{} $t_3$) can be written $t_1$ \arrow{} $t_2$ \arrow{} $t_3$.
\item A \emph{tuple type} has the form ($t_1$, $\cdots$, $t_k$) where $k\ge{}2$, which is equivalent to the type (, $\cdots$,) $t_1$ $\cdots$ $t_k$ where there are $k-1$ commas between the parenthesis.
It denotes the type of $k$-tuples with the first component of type $t_1$, the second component of type $t_2$ and so on.
\item A \emph{list type} has the form $\bracka{}t\brackz{}$, which is equivalent to the type $\bracka{}\brackz{}$ $t$. It denotes the type of lists with elements of the type $t$.
\end{enumerate}

Although the tuple and list types have special syntax, they are not different from user-defined types with equivalent functionality.

\hasdiff{(\arrow{}) is not a type constructor. The only way to write function types is through the infix notation described above.}

Expressions, patterns and types have a consistent syntax. If $t_i$ is the type of expression or pattern $e_i$, then the expressions
($\backslash$ $e_1$ \arrow{} $e_2$),
$\bracka{}e_1\brackz{}$
and ($e_1$, $e_2$) have the types
($t_1$ \arrow{} $t_2$), $\bracka{}t_1\brackz{}$ and ($t_1$, $t_2$),
respectively.

With one exception, the type variables in a type expression are all assumed to be universally quantified; there is no explicit syntax for universal quantification. For example, the type expression \texttt{a \arrow{} a} denotes the type $\forall$ $a$.$a$ \arrow{} $a$.
For clarity, however, we'll sometimes write quantification explicitly when discussing the types of \frege{} programs.

The exception referred to is in annotated patterns of the form $(p::t)$, when $t$ is not a function type and has to do with properties of the types system. This is explained in the section dealing with \hyperref[higher-rank]{higher rank polymorphism}.

\hasdiff{
There are no class assertions or contexts. Instead, type variables can carry \hyperref[constraint]{class constraints}.
}

\section{User-Defined Data Types} \index{data type}  \index{top level!declaration!data}

In this section, we describe algebraic and native data types (\texttt{data} declaration) and type synonyms (\texttt{type} declaration). These declarations may only appear at the top level of a module.

\subsection{Algebraic Data type Declaration} \index{data type!user defined} \index{data type!algebraic} \label{algdcl}

\begin{flushleft}
\rul{datadcl} \term{data} \nont{conid} \some{\nont{tyvar}} \sym{=} \nont{constructors} \opt{\term{where} \bracea{} \nont{decls} \bracez{}}\\
\rul{constructors} \liste{\nont{constructor}}{\sym{$|$}}\\
\rul{constructor} \nont{conid} \some{\nont{simpletype}}\gcom{traditional constructor}
  \alt\  \nont{conid} \bracea{} \liste{\nont{varid} ::\nont{type}}{,}\bracez{}\gcom{constructor with fields}\\
\end{flushleft}

An algebraic data type declaration introduces a new type and constructors for making values of that type and has the form:
\begin{quote}
\texttt{data} $T$ $u_1$ $\cdots$ $u_k$ \texttt{=} $K_1$ $t_{11}$ $\cdots$ $t_{1k_1}$ $|$ $\cdots$ $|$ $K_n$ $t_{n1}$ $\cdots$ $t_{nk_n}$
\end{quote}

This declaration introduces a new type constructor $T$ with constituent data constructors $K_1$, $\cdots$, $K_n$ whose types are given by
\begin{quote}
$\forall$ $u_1$ $\cdots$ $u_k$ . $t_{i1}$ \arrow{} $\cdots$ \arrow{} $t_{ik_i}$ \arrow{} $T$ $u_1$ $\cdots$ $u_k$
\end{quote}

The type variables $u_1$ through $u_k$ must be distinct and may appear in the $t_{ij}$; it is a static error for any other type variable to appear in the right hand side.

It is possible to reference the newly introduced type constructor $T$ on the right hand side of its own declaration, which allows to declare recursive types. For example, the following type is like the built-in list type:

\begin{code}
data List a = EmptyList | Cons a (List a)
\end{code}

The declaration can be read "A list of elements of type a is either the empty list or an element of type a "consed" to another list of elements of type a".

There are two special cases for user defined data types:
\begin{description}
\item [product types] are data types with only one constructor. The most prominent product types are tuples.
\item [enumerations] are data types where all constructors are constants, i.e. \exq{
data Colour = Black | Red | Yellow}
\end{description}

\subsubsection{Constructors with labeled fields} \label{fieldconstructor}

A different form for constructors is

\begin{quote}
$K_j$ \bracea{} $f_{j1}$ :: $t_{j1}$, $\cdots$, $f_{j{k_j}}$ :: $t_{j{k_j}}$ \bracez{}
\end{quote}

where the $f_ji$ are field labels and the $t_ji$ are the types. 
As before, the type of the constructor is

\begin{quote}
$\forall$ $u_1$ $\cdots$ $u_k$ . $t_{j1}$ \arrow{} $\cdots$ \arrow{} $t_{jk_j}$ \arrow{} $T$ $u_1$ $\cdots$ $u_k$
\end{quote}

Any number of constructors of an algebraic data type can employ the field list syntax. 
Constructors with and without field lists can be mixed.

Every field in one and the same constructor must have a different label, but the same label can appear in a different constructor of the same \term{data} definition. 
All fields with the same label that appear in the same \term{data} definition must have the same type.

The same field label with a possibly different type can appear in constructors of other types. 
This is because field names are only known locally in the type whose constructor introduced them. 
A name given to a field can thus in addition be used for a global variable.

For every field label appearing in an algebraic data type, 
the compiler defines automatically functions that extract a field from a value,
update or change a field in a value
\footnote{This is, of course, nondestructive update, i.e. a new value that differs only in the value for the field is created} 
and check, if the given value has a certain field.

\hasdiff{Field names are not known globally. Several forms of \hyperref[primexp]{primary expressions} deal with extraction, update and change of fields.}

The following box shows how the generated field functions look like.

\trans{\\
\begin{flushleft}
\textbf{data} T = A $t_{a}$ $|$  B \bracea{}$f_b$ :: $t_{b}$, $f_{bc}$ :: $t_{bc}$ \bracez{} $|$ C \bracea{}$f_{bc}$ :: $t_{bc}$, $f_c$ :: $t_c$\bracez{}\\
\end{flushleft}
translates to
\begin{flushleft}
\textbf{data} T = A $t_a$ $|$  B $t_b$  $t_{bc}$ $|$ C $t_{bc}$ $t_c$ \textbf{where}\\
\hspace{0.3in}// For each of the fields $f_b$, $f_ {bc}$ and $f_c$,\\ 
\hspace{0.3in}// 4 functions will be generated,\\
\hspace{0.3in}// we give here exemplary the ones for $f_{bc}$\\
\hspace{0.3in}$f_{bc}$ $v$ = \textbf{case} $v$ \textbf{of}\\
\hspace{0.5in}B \_ $f$ $\rightarrow$ $f$\\
\hspace{0.5in}C $f$ \_ $\rightarrow$ $f$\\
\hspace{0.3in}upd$f_{bc}$ $v$ $u$ = \textbf{case} $v$ \textbf{of}\\
\hspace{0.5in}B $a$ \_ $\rightarrow$ B $a$ $u$\\
\hspace{0.5in}C \_ $a$ $\rightarrow$ C $u$ $a$\\
\hspace{0.3in}chg$f_{bc}$ $v$ $g$ = \textbf{case} $v$ \textbf{of}\\
\hspace{0.5in}B $a$ $f$ $\rightarrow$ B $a$ ($g$ $f$)\\
\hspace{0.5in}C $f$ $a$ $\rightarrow$ C ($g$ $f$) $a$\\
\hspace{0.3in}has$f_{bc}$ $v$ = \textbf{case} $v$ \textbf{of}\\
\hspace{0.5in}B \_ \_ $\rightarrow$ \textbf{true}\\
\hspace{0.5in}C \_ \_ $\rightarrow$ \textbf{true}\\
\hspace{0.5in}A \_   $\rightarrow$ \textbf{false}
\end{flushleft}
The names of the upd..., chg... and has... functions can not be mentioned directly by the programmer, 
as they are picked by the compiler in order to avoid name clashes. There exist suitable  \hyperref[primexp]{primary expressions} to obtain and use the functions, though.
}

\subsubsection{Type Namespaces}

Each data type $T$ $u_1$ $\cdots$ $u_k$ is at the same time a namespace. 
This namespace will be populated with the declarations in the optional \texttt{where} clause of the \texttt{data} declarations.
Each item $i$ declared in the \texttt{where} clause of data type $T$ $u_1$ $\cdots$ $u_k$  can be accessed with the qualified name $T$.$i$ and, 
as explained in \autoref{memfunapp}, 
if $e$ is of type $T$ $u_1$ $\cdots$ $u_k$, 
then the expression $e$.$i$ is equivalent to ($T$.$i$ $e$).

\subsubsection{Predefined Algebraic Data Types} \label{predef}

As mentioned elsewhere, \frege{} has some builtin types that are supported with special syntax. Apart from that, they are not different from user defined algebraic datatypes.

\label{unittype}
The unit type $()$ is an enumeration with just one constant, which is also named $()$. The unit type is often the result type of impure functions that exist for their side effects.

\label{tupletypes} \index{tuples}
The tuple types are convenient product types for grouping a fixed number of  values that can have different types. Tuple types with 2 to 26 components are predefined.

\label{listtype} \index{list type}
The list type $\bracka{}a\brackz{}$ is one of the most used types in functional programming. As shown above, a similar type could be defined easily by the user. Although syntactic support as described under \autoref{listterm} would be missing, all expressions and patterns could be translated manually and written down without special syntax.

\subsubsection{Record Types} \label{recordtypes} \index{data type!record} \index{record} \index{record!type}
Records are data structures that can hold a fixed number of named values, called record components or record fields.
Like with tuples, the components of records may have different types.
Each record field must have a name that differs from all
other names used in the same record type.

Unlike other types, record types are not declared in a \texttt{data} declaration but spring into existence when they are mentioned explicitely or even implicetely, for example by writing a \hyperref[recordterm]{record term}.

\hasdiff{
Field names are not visible globally. The same field name can appear in different record types and there may be unrelated global or local bindings with the same name.

Also, record types (and record typed values) are independent of any other type. They're not doomed to live in the shadow of a data constructor, for example.
}

\paragraph*{Record Type Notation}
A record type has the form
\begin{quote}
\{ $f_1$ :: $t_1$, $\cdots$, $f_k$ :: $t_k$ \}
\end{quote}
where $k\ge 1$ and the order of fields $f_i$ is unimportant, i.e. all type expressions listing the same field/type pairs in arbitrary order denote the same type. Compilers may present record types in some canonical form, i.e. with field names sorted alphabetically.

A field/type pair $f$::$f$ can be abbreviated $f$. Note that the type variables scope over the entire record type, so that the type \exq{\{a, b::[a]\}} makes the type of field \texttt{b} depend on the type of field \texttt{a}.
It is recommended that one makes such dependencies explicit by sacrifying terseness.

To make life easier, you'll want to create a
\hyperref[typedcl]{type synonym} for your record types.


Record field access is described in \autoref{recaccess}, record update in \autoref{recupd}. Record constructors were introduced in \autoref{recordterm}, record patterns in \autoref{patterns}.

\subsection{Native Datatype Declaration} \index{data type} \index{data type!native} \index{data type!user defined} \label{nativedat}

\begin{flushleft}
\rul{datadcl} \term{data} \nont{conid} \sym{=} \term{native} \nont{nativeitem}\\
\rul{nativeitem} \nont{nativename} \oder{} \nont{stringliteral}
\rul{nativename} \nont{varid}
  \alt \nont{conid}
  \alt \nont{varid} \sym{.} \nont{nativename}
  \alt \nont{conid} \sym{.} \nont{nativename}
\end{flushleft}

A native datatype declaration \texttt{data} $T =$ \texttt{native} $N$ introduces a new type $T$ that makes the \java{} type $N$ available as abstract datatype. $N$ may be a primitive type like \texttt{int} or any reference type. It is recommended to use fully qualified \java{} names for the latter in order to avoid name clashes in the generated code.

For convenience, \java{} type names can be specified without quotes, as long as the components are valid \frege{} identifiers. This excludes names starting with underscores or names that contain the \$ sign and also \java{} generic type expressions like \texttt{java.util.List<Integer>}. Such \java{} types must be specified in the form of a string literal.

A frege compiler has no obligation to check the native type name for validity or to know anything about that type, except for the question whether it is a primitive type or a reference type.
This question will be resolved most easily by checking whether the native type name is one of \texttt{boolean}, \texttt{char}, \texttt{byte}, \texttt{short}, \texttt{int}, \texttt{long}, \texttt{float} or \texttt{double}.
If this is the case, the type will be regarded as primitive, otherwise as reference type.
In all other respects, however, the native type is opaque as far as \frege{} is concerned.
The generated code will contain the native type string and this will make the \java{} compiler complain if something is not in order.

Note that \texttt{void} is not a valid native type name since \texttt{void} is not a proper type.

For one and the same native type, more than one native datatype declaration can exist. However, the corresponding \frege{} types will be regarded as different types.

Native datatypes can be used like any other type, for example, they can serve as building blocks for algebraic data types, or be parts of function types. However, since there is no constructor nor any knowledge about the implementation of the type, no pattern matching on values of native datatypes is possible. Basic operations on the type can be introduced with \hyperref[nativefun]{native function declarations}.

\frege{} borrows all fundamental data types like characters, numbers, booleans and strings from \java{} via native datatype declarations. For an in depth discussion see \autoref{types}.

\subsection{Native Array Declaration} \index{data type} \index{data type!native array} \index{data type!user defined}

A native array declaration is a special form of a native datatype declaration that comes in two flavors. One is needed for arrays of \java{} values, the other for arrays of \frege{} values.

\begin{flushleft}
\rul{datadcl} \term{data} \nont{conid} \sym{=} \term{native} \bracka\brackz{} \nont{type} \gcom{where \emph{type} is native}
  \alt \term{data} \nont{conid} \opt{\nont{tyvar}} \sym{=} \term{native} \bracka\brackz{} \nont{type} \gcom{where \emph{type} is not native}\\
\end{flushleft}

The first form is needed when \java{} functions are to be used that take arrays as arguments or return arrays. Such arrays will most likely contain bare \java{} values.

Arrays of native values are further subdivided into arrays of primitive values and arrays of references. This is as complicated as the host language dictates it.

Let $P$ be the type constructor for a native datatype with primitive \java{} type. Then the declaration \texttt{data} $PA =$ \texttt{native} [] $P$ will predeclare the following functions in the namespace $PA$:

\begin{flushleft}
\texttt{get} :: $PA$ \arrow{} \texttt{Int} \arrow{} $P$ \gcom{element access}\\
\texttt{set} :: $PA$ \arrow{} \texttt{Int} \arrow{} $P$ \arrow{} \texttt{()} \gcom{destructive update}\\
\texttt{upd} :: $PA$ \arrow{} \texttt{Int} \arrow{} $P$ \arrow{} $PA$ \gcom{nondestructive update}\\
\texttt{length} :: $PA$ \arrow{} \texttt{Int} \gcom{get array length}\\
\texttt{new} :: \texttt{Int} \arrow{} $PA$ \gcom{array creation}\\
\end{flushleft}

Let $R$ be the type constructor for a native datatype based on a \java{} reference type. Then the declaration \texttt{data} $RA =$ \texttt{native} [] $R$ will predeclare the following functions in the namespace $RA$:

\begin{flushleft}
\texttt{get} :: $RA$ \arrow{} \texttt{Int} \arrow{} \texttt{Maybe} $R$ \gcom{element access}\\
\texttt{set} :: $RA$ \arrow{} \texttt{Int} \arrow{} \texttt{Maybe} $R$ \arrow{} \texttt{()} \gcom{destructive update}\\
\texttt{upd} :: $RA$ \arrow{} \texttt{Int} \arrow{} \texttt{Maybe} $R$ \arrow{} $RA$ \gcom{nondestructive update}\\
\texttt{elemAt} :: $RA$ \arrow{} \texttt{Int} \arrow{} $R$ \gcom{element access}\\
\texttt{length} :: $RA$ \arrow{} \texttt{Int} \gcom{get array length}\\
\texttt{new} :: \texttt{Int} \arrow{} $RA$ \gcom{array creation}\\
\end{flushleft}

The difference to the primitive type version is that \java{} arrays of references can contain \texttt{null} values. The \texttt{Maybe} type is used in the native interface to handle such values in a manner that excludes the possibility of null pointer exceptions. See the corresponding discussion in \autoref{nativefuns}.

The \texttt{elemAt} function is for the case when it is absolutely sure that the array does not contain \texttt{null} values. It is faster than \texttt{get} because construction and deconstruction of \texttt{Maybe} values is avoided. The price is the risk of a null pointer exception.

The existence of the member functions \texttt{get}, \texttt{upd} and \texttt{set} make it possible to use the \hyperref[arrayop]{primary expression syntax for arrays} with native arrays.

Native arrays are themselves native datatypes based on reference types. It is thus possible to declare multidimensional array types: \exq{
data Int = native int                   // int\\
data IntArray = native [] Int           // int[]\\
data IntArrayArray = native [] IntArray // int[][]\\
...
}

\paragraph*{Arrays with \frege{} values} The second form of the native array declaration creates a type constructor but does not predeclare any functions. It is included here for completeness only but is not intended for public use. The reason is that it suffices to have the most general polymorphic array type that is declared like this:
\begin{code}
data Array elem = native [] elem where
    // declaration of common array functions
    get :: Array a -> Int -> Maybe a
    upd :: Array a -> Int -> Maybe a -> Array a
    ...
\end{code}
Such a declaration is indeed already contained in the standard Prelude; it is described in more detail in \autoref{standardarray} and covers all imaginable array types for \frege{} values.

\subsection{Native Structure Types}

\begin{flushleft}
\rul{datadcl} \term{data} \nont{conid} \some{\nont{tyvar}} \sym{=} \term{native} \bracea{} \liste{\nont{fieldtyp}}{\sym{,}} \bracez{}
\end{flushleft}
\inmargin{write me!}

\subsection{Type Synonyms} \label{typedcl} \index{declaration!top level!type synonym}

\begin{flushleft}
\rul{typdcl} \term{type} \nont{conid} \some{\nont{tyvar}} \sym{=} \nont{type}
\end{flushleft}

A type synonym has the form
\begin{quote}
\texttt{type} $T$ $u_1$ $\cdots$ $u_k$ = $t$
\end{quote}
where $k\ge 0$ and the $u_i$ are the type variables occuring in the type expression $t$. It is a static error if $t$ contains a type variable not listed on the left hand side among the $u_i$. The $u_i$ must be bare type variables, no constraints are allowed, neither on the left hand side nor on the right hand side.

A type synonym $T_a$ depends on another type synonym $T_b$ if its right hand side mentions $T_b$ or another type synonym $T_c$ that in turn depends on $T_b$.

It is a static error if a type synonym $T$ depends on itself.
This means that cyclic definitions like \exq{
type T a = (a, X a)\\
type X a = T a\\}
are forbidden.

A type expression ($T$ $t_1$ $\cdots$ $t_k$) where $T$ is a type synonym is equivalent to the type expression $t$ of the right hand side of the declaration of $T$ where each variable $u_i$ in $t$ is replaced by the type expression $t_i$. Type synonyms cannot be partially applied in type signatures, it is a static error if during typechecking a type synonym declared with $k$ type variables is found, that is applied to less than $k$ type arguments. However, it is not required that the right hand side of a type synonym declaration is indeed a type; expansion may produce a partially applied type constructor or type synonym:

\begin{code}
type T key value = [(key, value)]
type X = T String

foo :: X Int
foo = [("x", 42)]
\end{code}

In the example above, expansion of type \texttt{X} produces a partially applied type synonym; to form a type one more argument is required.

Type synonyms are most often used to make type expressions more readable and programs more maintenable.

Another purpose is to create type constructors of a form that may be required by some class. In the example below, the idea of class \texttt{Keyed} is to provide operations for map like data structures. (Only the \texttt{lookup} operation is shown here for brevity.) Clearly, the type of \texttt{lookup} depends on the types of the key and the value, so the instantiated types should be types that are parameterized with two types. Moreover, the declaration dictates that the first type parameter be the key type and the second the value type.

Without type synonyms, we can make an instance for \texttt{Map1} only. If we need an instance for \texttt{Map2}, we're  out of luck (remember that \texttt{Map2} could have been imported from another package).

It's also not possible to use a list \texttt{[(key, value)]} as map, since the list type constructor is unary.

The type synonyms \texttt{Map2R} and \texttt{ListKV} make two pseudo type constructors that are suitable as \texttt{Keyed} instances.

\paragraph*{Example}:

\begin{code}
class Keyed keyed where
    lookup :: keyed key value -> key -> Maybe value

data Map1 k v = ....
data Map2 v k = .....

instance Keyed Map1 where ...

type Map2R a b = Map2 b a
instance Keyed Map2R where ...

type ListKV a b = [(a,b)]
instance Keyed ListKV where ...
\end{code}

\section{Type Classes and Overloading}
\subsection{Class Declarations} \label{classdcl} \index{class!declaration} \index{declaration!top level!class}

\begin{flushleft}
\rul{classdcl} \term{class} \nont{conid} \nont{classvar} \opt{\term{where} \bracea{} \nont{decls} \bracez{}}\\
\rul{classvar} \nont{classname} \sym{:} \nont{classvar}
  \alt \nont{varid}
\end{flushleft}

A \emph{class declaration} introduces a new class and the operations (\emph{class methods}) on it. A class declaration has the general form:

\begin{quote}
\texttt{class} $C$ \hspace{0.2cm} $S_1$:$\cdots$:$S_k$:$u$ \hspace{0.2cm} \texttt{where} \hspace{0.2cm} \emph{decls}
\end{quote}

This introduces a new class name $C$; the class variabe $u$ is scoped only over the class method signatures in the class body.

The $S_i$ denote the superclasses of $C$.
The superclass relation must not be cyclic. If $S$ is a superclass of $C$, then each type that is an instance of $C$ must also be an instance of $S$.

The class declaration introduces new \emph{class methods} in its \emph{decls} part.
The class methods are not only visible in the scope of the class, but also at the top (package) level.
Consequently, a class method can not have the same name as a top level definition or another class method.

A class method declaration is either

\begin{itemize}
\item a new operation introduced with a \hyperref[annotation]{type annotation}. The type given must mention the class variable and the class variable must not be constrained.

Optionally, a definition of the method can be given. The definition serves as a default implementation and will be used in instance definitions that give no instance specific implementation of that method.
\footnote{
For technical reasons, the expression defining a class or superclass method can be restricted to certain simple expression forms. For example, \texttt{let} or \texttt{case} expressions may not be allowed.
}
\item a definition of one of the class methods of a superclass. There must be no annotation for such methods. The type of the class method will be derived from that of the superclass method by replacing the superclasses' class variable with the current class variable.
\end{itemize}

No other declarations are permitted in the \emph{decls} part of a \texttt{class} declaration.

A \texttt{class} declaration without \texttt{where} clause is equivalent to one with an empty \emph{decls} part.
This can be useful for combining a collection of classes into a larger one that inherits all of the class methods in the original ones.
Nevertheless, even if a type is an instance of all the superclasses, it is not \emph{automatically} an instance of the new class. An instance declaration (with no \texttt{where} clause) is still required to make it one.

\paragraph*{Example} We give here a simlified version of some classes declared in the standard Prelude.

\label{classexample}
\begin{code}
class Eq eq where
    (==) :: eq -> eq -> Bool
    (!=) :: eq -> eq -> Bool
    a != b  =  if a == b then false else true

class Ord Eq:ord where
    (<)  :: ord -> ord -> Bool
    ...

class Enum Ord:enum where
    toInt   :: enum -> Int
    fromInt :: Int -> enum
    minval  :: enum
    a == b  =  (ord a) Eq.== (ord b)
    ...
\end{code}

The \texttt{Eq} class introduces two new overloaded operations, with a default implementation for the \texttt{(!=)} method that makes use of the \texttt{(==)} operation. The \texttt{Ord} class is a subclass of \texttt{Eq} and introduces more relational operations. The \texttt{Enum} class is declared as subclass of \texttt{Ord} and this makes it automatically also a subclass of \texttt{Eq}. Therefore, it is possible to give a default implementation for the \texttt{(==)} method. Note that in the definition the reference to \texttt{(==)} needs to be qualified. Without qualification, it would resolve to \texttt{Enum.==} and then the definition could not survive the type check, since it had to be established before that \texttt{Int} was an instance of \texttt{Enum}. But this couldn't be done before the type signature of all declarations in \texttt{Enum} were known and checked.

\paragraph*{Resolving Overloaded Methods}

In most cases, the compiler can deduce statically from the type of the operands of overloaded methods, which instances' implementation to use. For example in \texttt{(a == 0)} it can be inferred that \texttt{a} must be of the same type as \texttt{0}, namely \texttt{Int}, and the expression can be rewritten as \texttt{(a Int.== 0)}.

However, consider the follwoing example: \begin{code}
class Num number where
  (*) :: number -> number -> number

sqr n = n*n
\end{code}

Here, in \texttt{sqr}, the type of \texttt{n} is not known exactly. It is only known and enforced that it must be some type that is an instance of \texttt{Num}. In such cases, a dynamic dispatch based on the first argument of the method (i.e. the left operand of binary operators) will be done at runtime.

There are two cases where this convention does not work:

\begin{itemize}
\item The class method is not a function but just a value and thus has no arguments. In the example on page \pageref{classexample} above, this is the case for \texttt{Enum.minval}.
\item The type of the first argument of the class method is not the class variable. In the example above, this is the case for \texttt{Enum.fromInt}.
\end{itemize}

In this case, it is required that an extra pseudo argument is passed that is solely used for selecting the correct implementation at runtime.

For example, if a type with type constructor $E$ is an instance of \texttt{Enum}, we can retrieve the \texttt{minval} specific for $E$ by writing $E$.\texttt{minval}. Alternatively, if we have a value $e$ of type \texttt{Enum:x} (some type that is an instance of \texttt{Enum}), we can get the \texttt{minval} specific to that type by writing $e$.\texttt{minval} or (\texttt{minval} e).

Consider also the following example that shows a generic successor function that works for every type that is an instance of class \texttt{Enum}:

\begin{code}
succ :: Enum:e -> Enum:e
succ e = fromInt e (toInt e + 1)  // OR e.fromInt (e.toInt+1)
\end{code}

The first argument to \texttt{fromInt} is the pseudo argument that selects the appropriate \texttt{fromInt} implementation at runtime, but is not itself passed to the selected function which, according to the type signature, takes only 1 argument of type \texttt{Int}.

It must be noted that nevertheless in \frege{} it is not possible, for example, to write a polymorphic function that computes the sum of a list for any numeric type. For, of what type should the zero result be in case of an empty list? A generic sum function is thus only possible for non-empty lists.

\subsection{Instance Declarations} \label{instdcl} \index{instance!declaration} \index{declaration!top level!instance}

\begin{flushleft}
\rul{instdcl} \term{instance} \nont{classname} \nont{type} \opt{\term{where} \bracea{} \nont{decls} \bracez{}}
\end{flushleft}

An \emph{instance declaration} introduces an instance of a class. Let
\begin{quote}
\texttt{class} $C$ $u$ \texttt{where \{} \nont{cbody} \texttt{\}}
\end{quote}
be a class declaration.
The general form of the corresponding instance declaration is:
\begin{quote}
\texttt{instance} $C$ ($T$ $t_1$ $\cdots$ $t_k$) \texttt{where \{} \nont{decls} \texttt{\}}
\end{quote}
where $k \ge 0$. If $T$ is a type synonym, the expansion of the type expression must eventually lead to a type application of a type constructor or to a function type.

All type variables occuring in the $t_i$ must be distinct from all type variables used in the class body and from the class variable $u$.
\footnote{Since instances can be declared for classes introduced in imported packages, it is therefore advised to use unusual type variable names (e.g. no single letters) in the class declaration.}

There may be at most one instance per type class and type constructor. Because of this restriction, it is usually a good idea to design the instance as general as possible. The most general case is when the $t_i$ are all distinct type variables.

\paragraph*{Example}
\begin{code}
    class C this where ...
    instance C [Int] where ...   // ok, but quite restrictive
    instance C [a]   where ...   // ERROR, [] is already an instance of C
    instance C (a,a) where ...   // ok, but restricted
    instance C (a,b,c) where ... // most general instance
\end{code}
\hasdiff{It is not required that the type expression contains only type variables besides the type constructor.}

Implementations of class methods and not yet implemented superclass methods are searched in the scope of the instantiated type and in the instance declarations.
It is an error if both sources contain an imlplementation for a class method. It is also an error if none of them contains an implementation when there is no default implementation.

Annotations may be given; they will be checked to match the computed type of the instance method.
This type is obtained by substituting the type expression describing the instantiated type for every occurence of the class variable in the annotation of the class method. In addition, during typecheck, it will be checked that the definition of the instance method indeed has the computed type, as usual.

It is also possible to implement a class method with a \hyperref[nativefun]{native function}.

If a class method has a default implementation but no instance specific implementation is found, the source code of the default implementation is taken to construct an instance specific implementation.

Each declaration of a class method that comes from the instance declarations or from a default implementation is linked back to the namespace of the instantiated type. This is so that implementations of class operations \emph{classop} specific for type $T$ can be accessed under the qualified name $T$.\emph{classop}.

It is important to recall that declarations in where clauses of instance or data type declarations live in the scope of the data type or the instance. This has an influence of how unqualified identifiers are resolved. If the name is found in the data or instance context, then it will be resolved to that item and not to the global one. Consider:

\begin{code}
    data My a = My a
    instance Show (My Show:d) where
        show (My item) = "My " ++ show item       // ERROR!
\end{code}

This will produce a rather cryptic error message like
\begin{code}
line 34 (E) Cannot unify inferred type  My Show:<4212>
        with type variable d
        in expression  item
        (maybe your annotation is too polymorphic?)
\end{code}

Why does it infer type (\texttt{My Show}:$x$) for expression \texttt{item} instead of just \texttt{Show:}$d$? Because the unqualified variable \texttt{show} resolves to the instance of \texttt{show} that lives in the instance scope, and this has a (computed) type of \exq{show :: My Show:d -> String}
Therefore, the type of \texttt{item} is inferred as \texttt{My Show}:$x$. But this implies that the \texttt{show} function just declared would have a type of \exq{show :: My (My Show:x) -> String}
which is clearly not compatible with the type above, because the (implicit) annotation is more general than the inferred type.
The remedy is to write either (\texttt{Show.show item}) or just \texttt{item.show} to avoid referring to the \texttt{My.show} function.

\paragraph{Instantiating a Class Hierarchy} with a single instance declaration. It is possible to declare an instance for a subclass and hence to make the instantiated type also an instance of all superclasses without giving explicit instance declarations for the superclasses, provided that
\begin{itemize}
\item the instance declaration contains implementations for the required class methods of the superclasses or
\item the subclass gives default implementations for the superclasses' required methods.
For example, the type class \texttt{Ord} is a subclass of \texttt{Eq}, but has a default implementation for \texttt{(Eq.==)}.
Class \texttt{Eq} itself has a default implementation for \texttt{(Eq.!=)}.
Thus it is possible to declare an \texttt{Ord} instances without caring about the \texttt{Eq} methods.
\end{itemize}

\subsection{Derived Instances}

For the Prelude classes \texttt{Eq}, \texttt{Ord}, \texttt{Enum} and \texttt{Show} it is possible to \emph{derive} instances automatically.

\begin{flushleft}
\rul{derivedcl} \term{derive} \nont{classname} \nont{type}
\end{flushleft}

Derived instances provide convenient commonly-used operations for user-defined datatypes.
For example, derived instances for datatypes in the class \texttt{Eq} define the operations \texttt{==} and \texttt{!=}, freeing the programmer from the need to define them. The precise details of how
the derived instances are generated for each of these classes are given below,
including a specification of when such derived instances are possible.

\trans{
A valid \texttt{derive} declaration is translated like this:\\
\begin{tabular}{lcl}
\textbf{derive} $C$ $t$ & $=$ & \textbf{instance} $C$ $t$ \textbf{where}\\
& & \hspace{1cm}$decls_C$\\
\end{tabular}
\par where $decls_C$ are compiler generated declarations whose concrete content depends on $C$ and the structure of $t$.
}

\subsubsection{Derived Instances for Eq}

Instances of \texttt{Eq} are types whose values can be compared for equality. An \texttt{Eq} instance can be derived for user defined algebraic data types and record types.

\trans{
Let $P$ be a $n$-ary $(n \ge 0)$ type constructor for a product type with $k$-ary $(k \ge 0)$ data constructor $C$:
\begin{flushleft}
\textbf{data} $P$ $u_1$ $\cdots$ $u_n$ = $C$ $ct_1$ $\cdots$ $ct_k$\\
\hspace{1cm}Then\\
\textbf{derive} \texttt{Eq} ($P$ $t_1$ $\cdots$ $t_n$)\\
\hspace{1cm}is equivalent to:\\
\textbf{instance} \texttt{Eq} ($P$ $t_1$ $\cdots$ $t_n$) \textbf{where}\\
\hspace{0.3cm}$C$ $a_1$ $\cdots$ $a_k$ \texttt{==} $C$ $b_1$ $\cdots$ $b_k$ \texttt{=}\\
\hspace{0.6cm}\textbf{true} \texttt{\&\&} $a_1$ \texttt{Eq.==} $b_1$ \texttt{\&\&} $\cdots$ \texttt{\&\&} $a_k$ \texttt{Eq.==} $b_k$\\
\end{flushleft}
}
The type expressions $t_1, \cdots, t_n$ must be choosen so that all component types $ct_1, \cdots, ct_k$ will be instances of \texttt{Eq} or type variables constrained with \texttt{Eq}.

The generated expression returns \textbf{true} if all subcomponents of the left operand are pairwise equal with the corresponding subcomponents of the right operand, otherwise the result is  \textbf{false}.

Note that the special case $k=0$ is trivial: such a type has only one value $C$ and the derived \texttt{==} returns always \textbf{true}.

A record type \{$f_1$::$ct_1$, $\cdots$, $f_k$::$ct_k$\} is a product type; the patterns used in the derived method will be in the form \{$f_1=a_1$, $\cdots$, $f_k=a_k$\} and \{$f_1=b_1$, $\cdots$, $f_k=b_k$\}.

The case gets only marginally more complex with sum types.
\trans{
Let $S$ be a $n$-ary $(n \ge 0)$ type constructor for a sum type with $m$  $(m \ge 2)$ data constructors $C_1, \cdots, C_m$ and arities $k_1, \cdots, k_m$:
\begin{flushleft}
\textbf{data} $S$ $u_1$ $\cdots$ $u_n$ = $C_1$ $ct_{1_1}$ $\cdots$ $ct_{k_1}$ $| \cdots |$ $C_m$ $ct_{m_1}$ $\cdots$ $ct_{k_m}$\\
\hspace{1cm}Then\\
\textbf{derive} \texttt{Eq} ($S$ $t_1$ $\cdots$ $t_n$)\\
\hspace{1cm}is equivalent to:\\
\textbf{instance} \texttt{Eq} ($S$ $t_1$ $\cdots$ $t_n$) \textbf{where}\\
\hspace{0.33cm}$a$ \texttt{==} $b$ \texttt{=} \textbf{case} $(a, b)$ \textbf{of}\\
\hspace{0.66cm}$(C_1 a_1 \cdots a_{k_1}, C_1 b_1 \cdots b_{k_1})$\\
\hspace{2cm}$\rightarrow$ \textbf{true}  \texttt{\&\&} $a_1$\texttt{.==} $b_1$ \texttt{\&\&} $\cdots$ \texttt{\&\&} $a_{k_1}$\texttt{.==} $b_{k_1}$\\
\hspace{0.66cm}$\cdots$\\
\hspace{0.66cm}$(C_m a_1 \cdots a_{k_m}, C_m b_1 \cdots b_{k_m})$\\
\hspace{2cm}$\rightarrow$ \textbf{true} \texttt{\&\&} $a_1$\texttt{.==} $b_1$ \texttt{\&\&} $\cdots$ \texttt{\&\&} $a_{k_m}$\texttt{.==} $b_{k_m}$\\
\hspace{0.66cm}\_ $\rightarrow$ \textbf{false}
\end{flushleft}
}
The type expressions $t_1, \cdots, t_n$ must be choosen so that all component types $ct_{1_1}, \cdots, ct_{k_m}$ will be instances of \texttt{Eq} or type variables constrained with \texttt{Eq}.

The expression $a$ \texttt{==} $b$ evaluates to \textbf{true} if both $a$ and $b$ were constrcuted with the same data constructor and their corresponding subcomponents are pairwise equal.

\subsubsection{Derived Instances for Ord}
The \texttt{Ord} class is used for totally ordered datatypes. It is a subclass of \texttt{Eq} and inherits the operations \texttt{==} and \texttt{!=}. It defines one new operation \texttt{<=>} that must be implemented by all instances, and operations \texttt{<}, \texttt{<=}, \texttt{>}, \texttt{>=}, \texttt{max} and \texttt{min} in terms of \texttt{<=>}.

The compare function \texttt{<=>} compares two values and returns a result of type \texttt{Ordering}, which is defined as
\begin{code}
    data Ordering = Lt | Eq | Gt
\end{code}

Instances of \texttt{Ord} can be derived for user defined algebraic data types and record types. Derived instances for sum types make use of the Prelude function
\begin{quote}
\texttt{constructor} :: $any \rightarrow$ \texttt{Int}
\end{quote}
which returns 0 for values of product types, function types and native types and the index of the constructor for sum types. The constructors are numbered starting from 0 in the order they appear in the \hyperref[datadcl]{data definition}.

\trans{
Let $P$ be a $n$-ary $(n \ge 0)$ type constructor for a product type with $k$-ary $(k \ge 1)$ data constructor $C$:
\begin{flushleft}
\textbf{data} $P$ $u_1$ $\cdots$ $u_n$ = $C$ $ct_1$ $\cdots$ $ct_k$\\
\hspace{1cm}Then\\
\textbf{derive} \texttt{Ord} ($P$ $t_1$ $\cdots$ $t_n$)\\
\hspace{1cm}is equivalent to:\\
\textbf{instance} \texttt{Ord} ($P$ $t_1$ $\cdots$ $t_n$) \textbf{where}\\
\hspace{0.33cm}$(C a_1 \cdots a_k)$ \texttt{<=>} $(C b_1 \cdots b_k)$ \texttt{=} \textbf{case} $a_1$\texttt{.<=>} $b_1$ \textbf{of}\\
\hspace{0.66cm}\texttt{Eq} $\rightarrow$\\
\hspace{1cm}$\cdots$\\
\hspace{1.33cm}\textbf{case} $a_{k-1}$\texttt{.<=>} $a_{k-1}$ \textbf{of}\\
\hspace{1.66cm}\texttt{Eq} $\rightarrow$ $a_k$\texttt{.<=>} $b_k$\\
\hspace{1.66cm}$r_{k-1}$ $\rightarrow$ $r_{k-1}$\\
\hspace{1cm}$\cdots$\\
\hspace{0.66cm}$r_1$ $\rightarrow$ $r_1$\\
\end{flushleft}
}
The type expressions $t_1, \cdots, t_n$ must be choosen so that all component types $ct_1, \cdots, ct_k$ will be instances of \texttt{Ord} or type variables constrained with \texttt{Ord}. The generated expression compares the components $a_i$, $b_i$ from 1 to $k-1$; the first result $r_i$ that does not signify equality is the result of the overall comparison. Otherwise, if all component pairs up to ${k-1}$ compare equal, the result is the ordering of the last component pair, $a_k$\texttt{.<=>} $b_k$.

The translation does not handle the case of the trivial product type. Such a type will have an implementation of \texttt{<=>} that always returns \texttt{Ordering.Eq}.

It follows the translation for sum types.

\trans{
Let $S$ be a $n$-ary $(n \ge 0)$ type constructor for a sum type with $m$  $(m \ge 2)$ data constructors $C_1, \cdots, C_m$ and arities $k_1, \cdots, k_m$:
\begin{flushleft}
\textbf{data} $S$ $u_1$ $\cdots$ $u_n$ = $C_1$ $ct_{1_1}$ $\cdots$ $ct_{k_1}$ $| \cdots |$ $C_m$ $ct_{m_1}$ $\cdots$ $ct_{k_m}$\\
\hspace{1cm}Then\\
\textbf{derive} \texttt{Ord} ($S$ $t_1$ $\cdots$ $t_n$)\\
\hspace{1cm}is equivalent to:\\
\textbf{instance} \texttt{Ord} ($S$ $t_1$ $\cdots$ $t_n$) \textbf{where}\\
\hspace{0.33cm}$a$ \texttt{<=>} $b$ \texttt{=} \textbf{case} \texttt{constructor} $a$ \texttt{Int.<=>} \texttt{constructor} $b$ \textbf{of}\\
\hspace{0.66cm}\texttt{Eq} $\rightarrow$ \textbf{case} $(a,b)$ \textbf{of}\\
\hspace{1cm}$alt_1$\\
\hspace{1cm}$\cdots$\\
\hspace{1cm}$alt_m$\\
\hspace{0.66cm}$r_0$ $\rightarrow$ $r_0$
\end{flushleft}
where each of the alternatives $alt_i$ has a form that depends on the arity of the constructor $C_i$:
\begin{flushleft}
\hspace{0.3cm}$(C_i, C_i) \rightarrow$ \texttt{Eq}\hspace{\fill}for nullary $C_i$\\
\hspace{0.3cm}$(C_i a_1, C_i b_1) \rightarrow a_1$\texttt{.<=>} $b_1$ \hspace{\fill}for unary $C_i$\\
\hspace{0.3cm}$(C_i a_1 \cdots a_{k_i}, C_i b_1 \cdots b_{k_i}) \rightarrow$ \hspace{\fill}for $C_i$ with arity $k_i \ge 2$\\
\hspace{1cm}$(a_1, \cdots, a_{k_i})$\texttt{.<=>} $(b_1, \cdots, b_{k_i})$
\end{flushleft}
}
The type expressions $t_1, \cdots, t_n$ must be choosen so that all component types $ct_{1_1}, \cdots, ct_{k_m}$ will be instances of \texttt{Ord} or type variables constrained with \texttt{Ord}.

The comparision first sorts out the cases where the constructors are not the same; the result in such a case is the ordering of the constructors. The remaining $m$ cases compare nullary constructors equal to themselves, values with unary constructors compare just like the components compare and values with constructors of higher arity compare like the tuples constructed from their components would compare when $k_i$-ary tuples had a derived instance of \texttt{Ord}.

\subsubsection{Derived Instances for Enum}

The \texttt{Enum} class is for algebraic datatypes that have only nullary constructors.
It provides conversion from and to \texttt{Int} values, successor and predecessor functions and the operation ($a$ \texttt{..} $b$) that constructs a list of all values $e$ where $a \le e \le b$. \texttt{Enum} is a subclass of \texttt{Ord}.

A trivial type can be an instance of \texttt{Enum}.
\trans{
Let $T$ be a trivial type:
\begin{flushleft}
\textbf{data} $T$ = $C$\\
\hspace{1cm}Then\\
\textbf{derive} \texttt{Enum} $T$\\
\hspace{1cm}is equivalent to:\\
\textbf{instance} \texttt{Enum} $T$ \textbf{where}\\
\hspace{0.5cm}minval = $C$; maxval = $C$; ord $C$ = 0; from 0  = $C$\\
\end{flushleft}
}

Product types with arity $k>0$ cannot be instances of \texttt{Enum}. It remains to show the translation for those sum types that can be instances of \texttt{Enum}.

\trans{
Let $S$ be a sum type with $m$ $(m \ge 2)$ nullary constructors:
\begin{flushleft}
\textbf{data} $S$ = $C_1 | \cdots | C_m$\\
\hspace{1cm}Then\\
\textbf{derive} \texttt{Enum} $S$\\
\hspace{1cm}is equivalent to:\\
\textbf{instance} \texttt{Enum} $S$ \textbf{where}\\
\hspace{0.5cm}minval = $C_1$; maxval = $C_m$\\
\hspace{0.5cm}ord $e$ = \textbf{case} $e$ \textbf{of}\\
\hspace{1.0cm}$C_1 \rightarrow$ 0\\
\hspace{1.0cm}$\cdots$\\
\hspace{1.0cm}$C_m \rightarrow$ $m-1$\\
\hspace{0.5cm}from $i$  = \textbf{case} $i$ \textbf{of}\\
\hspace{1.0cm}0 $\rightarrow C_1$\\
\hspace{1.0cm}$\cdots$\\
\hspace{1.0cm}$m-1$ $\rightarrow C_m$\\
\end{flushleft}
}
Note that the construct $m-1$ will be substituted by the appropriate integer constant. The application ($S$\texttt{.from} $i$) is undefined for $(i<0)$ or $(i \ge m)$. For all $C_i$ it is the case that $S$\texttt{.from} $C_i$\texttt{.ord ==} $C_i$

\subsubsection{Derived instances for Show}

The type class \texttt{Show} is for types whose values can represented as character strings. It can be derived for any algebraic data type.

\trans{
Let $S$ be a $n$-ary $(n \ge 0)$ type constructor for a type with $m$  $(m \ge 1)$ data constructors $C_1, \cdots, C_m$ and arities $k_1, \cdots, k_m$:
\begin{flushleft}
\textbf{data} $S$ $u_1$ $\cdots$ $u_n$ = $C_1$ $ct_{1_1}$ $\cdots$ $ct_{k_1}$ $| \cdots |$ $C_m$ $ct_{m_1}$ $\cdots$ $ct_{k_m}$\\
\hspace{1cm}Then\\
\textbf{derive} \texttt{Show} ($S$ $t_1$ $\cdots$ $t_n$)\\
\hspace{1cm}is equivalent to:\\
\textbf{instance} \texttt{Show} ($S$ $t_1$ $\cdots$ $t_n$) \textbf{where}\\
\hspace{0.5cm}\texttt{show v =} \textbf{case} \texttt{v} \textbf{of}\\
\hspace{1cm}$C_1 a_1 \cdots a_{k_1} \rightarrow$ \texttt{"}$C_i$\texttt{" ++ " "}\\
\hspace{2cm}\texttt{ ++} $a_1$\texttt{.showsub ++} $\cdots$ \texttt{" " ++} $a_k$ \texttt{.showsub}\\
\hspace{1cm}$\cdots$\\
\hspace{1cm}$C_m a_1 \cdots a_{k_m} \rightarrow \cdots$\\
\hspace{0.5cm}\texttt{showsub} $C_i$ \texttt{=} \texttt{"}$C_i$\texttt{"}\hspace{\fill}for each $i$ where $k_i=0$\\
\hspace{0.5cm}\texttt{showsub} $C_i a_1 \cdots a_{k_i}$ \texttt{=} \hspace{\fill}for each $i$ where $k_i>0$\\
\hspace{2cm}\texttt{"(" ++ show (} $C_i a_1 \cdots a_{k_i}$\texttt{) ++ ")"}
\end{flushleft}
}
The type expressions $t_1, \cdots, t_n$ must be choosen so that all component types $ct_{1_1}, \cdots, ct_{k_m}$ will be instances of \texttt{Show} or type variables constrained with \texttt{Show}.

The derived \texttt{show} function creates a textual representation of a value that will be a syntactically valid constructor application if the \texttt{Show} instances of the subcomponents also produce syntactically correct text. The \texttt{showsub} function shows the value enclosed in parenthesis if it is more complex than just a nullary constructor.

The translation above is equally valid for product and sum types. Types that enjoy special syntactic support (list types, tuples, records and the unit type) have also special \texttt{Show} instances whose translation is omitted for brevity. Suffice it to say that these instances will reproduce the familiar textual representations, i.e. the expression \texttt{show (1,2)} will produce \texttt{"(1, 2)"} and not \texttt{"(,) 1 2"}.

\section{Nested Declarations} \label{decl}

The declarations described here can appear at the top level or in the scope of a class  declaration, instance declaration or datatype declaration. With the exception of the \hyperref[nativefun]{native function declaration} they can also appear in a \hyperref[letexpr]{\texttt{let} expression}. In fact, this are the only declarations allowed in \texttt{let} expressions and \texttt{where} clauses that are part of expressions. (Such \texttt{where} clauses will be transformed to \texttt{let} expressions ulitmatively.)

\begin{flushleft}
\rul{decl} \nont{annotation} \gcom{Type Annotation}
  \alt \nont{binding}        \gcom{Function or Pattern Binding}
  \alt \nont{nativefun}      \gcom{Native Function Declaration}
\end{flushleft}

\subsection{Type Annotations} \label{annotation}

A \emph{type annotation} specifies the type of a variable.

\begin{flushleft}
\rul{annotation} \nont{annoitem} \sym{::} \nont{type}\\
\rul{annoitem} \nont{varid} \oder{} \nont{symop} \oder{} \nont{unop}
\end{flushleft}

A type annotation has the form
\begin{quote}
$v$ \texttt{::} $t$
\end{quote}
where $v$ is the annotated item which may be a variable or an unary or binary operator symbol. (To simplify matters, we use the term \emph{variable} in place of \emph{annotated item} in the discussion that follows.)

Except for class methods, annotated variables $v$ must have also a value binding, and the binding must appear in the same declaration list that contains the type signature; i.e. it is invalid
to give a type signature for a variable bound in an outer scope. Moreover, it is invalid to give more than one type signature for one variable, even if the signatures are identical.

As mentioned in \autoref{typesyntax}, every type variable appearing in a signature is universally quantified over that signature, and hence the scope of a type variable is limited to the type expression that contains it.
For example, in the following declarations \exq{
f :: a -> a \\
f x = x :: a // invalid \\
}
the \texttt{a}'s in the two type expressions are quite distinct.
Indeed, these declarations contain a static error, since \texttt{x} does not have type $\forall$ $a$.$a$ but is dependent on the function type.

If a given program includes a signature for a variable $f$ , then each use of $f$ is treated as having the declared type.
It is a static error if the same type cannot also be inferred for the defining occurrence of $f$.

If a variable $f$ is defined without providing a corresponding type signature declaration, then each use of $f$ outside its own declaration group (see \autoref{declgroup}) is treated as having the corresponding inferred, or \emph{principal} type.
However, to ensure that type inference is still possible, the defining occurrence, and all uses of $f$ within its declaration group must have the same monomorphic type (from which the principal type is obtained by generalization, as described in \autoref{generalization}).

For example, if we define \exq{
sqr x = x*x}
then the principal type is \texttt{sqr} :: $\forall$ $a$.\texttt{Num:}$a \rightarrow{}$ \texttt{Num:}$a$, which allows applications such as \texttt{sqr 5} or \texttt{sqr 0.1}.
It is also valid to declare a more specific type, such as \exq{
sqr :: Int $\rightarrow$ Int} but now appications such as \texttt{sqr 0.1} are invalid. Type signatures such as \exq{
sqr :: Num:a $\rightarrow$ Num:b\\
sqr :: a $\rightarrow$  a} are invalid, as they are more general than what is warranted by the definition.

However, there are certain cases where the type checker infers a type that is not the most general one possible for the definition given. In such cases, an annotation can be used to specify a type more general than the one that would be inferred. Consider this rather pathological example:
\begin{code}
data T a = K (T Int) (T a)
f :: T a -> a
f (K x y) = if f x == 1 then f y else undefined
\end{code}
If we remove the annotation, the type of \texttt{f} will be inferred as \texttt{T Int -> Int} due to the first recursive call for which the argument to \texttt{f} is \texttt{T Int}.

To sum it up, there are four possible uses of type annotations:
\begin{enumerate}
\item Declaration of a new class method, as described in \autoref{classdcl}.
\item Specifying a more restricted type than the principal type.
\item Specifying a more general type than the inferred type. Please observe that, even if the type inference algorithm is not able to infer the most general type from a definition, it is still able to check whether the type signatur supplied is valid. Therefore, type annotations cannot be used to lie about the type of a variable.
\item Specifying a type that is identical to the type that would have been inferred. This may be useful for documentation purposes.
\end{enumerate}

\hasdiff{It is not possible to annotate several items in a single annotation.}

\subsection{Function and Pattern Bindings} \label{binding}

\begin{flushleft}
\rul{binding} \nont{lhs} \nont{rhs}\\
\rul{lhs} \nont{funlhs} \oder{} \nont{pattern}\\
\rul{funlhs} \nont{varid} \more{\nont{pterm}}
  \alt \nont{pconapp} \nont{lexop} \nont{pconapp}\\
\rul{rhs} \sym{=} \nont{expr} \opt{\term{where} \bracea{} \nont{decls} \bracez{}}
  \alt \nont{guarded-exs}  \opt{\term{where} \bracea{} \nont{decls} \bracez{}} \gcom{see \autoref{caseex} for syntax of \nont{guarded-exs}}
\end{flushleft}

We distinguish two cases of bindings: If the left hand side is neither a constructor application nor an application of the unary operator \texttt{!}\footnote{This further exception in an already complicated rule can only be justified by the fact that one seldom wants to redefine the \texttt{!} function, but rather frequently wants to introduce strict pattern bindings.}, and if it can be interpreted as a variable applied to one or more patterns, or as a binary operator applied to two patterns in infix notation, we call it a function binding, otherwise it is a pattern binding. Thus, a left hand side like $v$@$p$ or $m$\symbol{126}\#$re$\#, though making a valid pattern, will be treated as function bindings (i.e. definitions of the operators \texttt{@} and \texttt{\symbol{126}}). On the other hand, $x$:$xs$ is a pattern binding, since this notation is special syntax for a constructor application, and \texttt{!}$x$ is also a (strict) pattern binding.

\subsubsection{Function bindings} \label{fundef}

A function binding binds a variable (or operator) to a function value.
The general form of a function binding for variable $x$ is:
\begin{quote}
\begin{flushleft}
$x$ $p_{11}$ $\cdots$ $p_{1k}$ $match_1$\\
$\cdots$\\
$x$ $p_{n1}$ $\cdots$ $p_{nk}$ $match_n$\\
\end{flushleft}
\end{quote}
where $k\ge 1$, $n\ge 1$ and each $p_{ij}$ is a pattern and the matches $match_i$ are just like the matches in \hyperref[caseex]{case expressions}.

All clauses defining a function must be contiguous, and the number of patterns in each clause must be the same. The set of patterns corresponding to each match must be linear, that is, no variable is allowed to appear more than once in the set.

\trans{The general binding form for functions is semantically equivalent to the equation
\begin{flushleft}
$x$ = $\backslash{}x_1 \cdots{} \backslash{}x_k \rightarrow{} $\textbf{case} $(x_1, \cdots, x_k)$ \textbf{of}\\
\hspace{2cm}$(p_{11}, \cdots, p_{1k})$ $match_1$\\
\hspace{2cm}$\cdots$\\
\hspace{2cm}$(p_{n1}, \cdots, p_{nk})$ $match_n$\\
\end{flushleft}
where the $x_i$ are new identifiers.
}

Note that several clauses defining a function count as a single  declaration. While definitions of different functions may appear in any order without changing the meaning of the program, this is not true for the clauses of a function definition. On the contrary, because of the translation given above and the semantics of \texttt{case} expressions, their order is quite important and cannot usually be changed without changing also the meaning of the program.

\subsubsection{Pattern bindings} \label{patdef}

A \emph{pattern binding} binds all variables contained in the pattern on the left hand side to values. It is a static error if the pattern does not contain any variables. The pattern is matched against the expression on the right hand side only when one of the bound variables needs to be evaluated. In any case, the expression on the right hand side will be evaluated at most once, and for each bound variable the match is performed at most once.

This default lazy semantics can be overridden by using strict patterns (see page \pageref{strict pattern}). A strict variable will be evaluated as soon as it is bound to a value. This may cause other variables on which the strict variable depends to be evaluated, too.

A \emph{simple} pattern binding has the form $v$ = $e$, where $v$ is just a variable. No actual pattern matching is needed in this case. The evaluation of $v$ will cause evaluation of $e$ and the resulting value is the value of $v$. If $v$ is not evaluated, $e$ will also not be evaluated.

The \emph{general} form of a pattern binding is $p$ $match$, where $match$ is the same structure as for function bindings above, which in turn is the one used in case expressions; in other words, a pattern binding is:
\begin{quote}
\begin{flushleft}
$p$ $|$ $g_1$ = $e_1$ $|$ $g_2$ = $e_2$ $|\cdots$ $|$ $g_m$ = $e_m$\\
\hspace{0.3cm}\textbf{where} \{ $decls$ \}
\end{flushleft}
\end{quote}
\trans{The general pattern binding above is semantically equivalent to the following:
\begin{flushleft}
$x$ = \textbf{let} \{ $decls$ \} \textbf{in}\\
\hspace{1cm}\textbf{case} () \textbf{of} \{ () $|$ $g_1$ = $e_1$ $|$ $g_2$ = $e_2$ $|\cdots$ $|$ $g_m$ = $e_m$ \}\\
$v_1$ = \textbf{case} $x$ \textbf{of} \{ $p \rightarrow{} v_1$ \}\\
$\cdots$\\
$v_k$ = \textbf{case} $x$ \textbf{of} \{ $p \rightarrow{} v_k$ \}\\
\end{flushleft}
where $k\ge 1$ and the $v_i$ are the variables occuring in the pattern $p$ and $x$ is a variable not used elsewhere.
}

\subsection{Static Semantics of Function and Patern Bindings} \label{letsemantics}

The static semantics of the function and pattern bindings of a \texttt{let} expression or \texttt{where} clause are discussed in this section.

\subsubsection{Dependency Analysis} \label{declgroup}

In general the static semantics are given by the normal Hindley-Milner inference rules.
A \emph{dependency analysis} transformation is first performed to enhance polymorphism.
Two variables bound by value declarations are in the same \emph{declaration group} if their bindings are mutually recursive (perhaps via some other declarations that are also part of the group).

Application of the following rules causes each \texttt{let} or \texttt{where} construct (including the (implicit) \texttt{where} defining the top level bindings in a module) to bind only the variables of a single declaration group, thus capturing the required dependency analysis:

\begin{enumerate}
\item The order of function and pattern bindings in \texttt{where}/\texttt{let} constructs is irrelevant.
\item \textbf{let} \{ $d_1$; $d_2$ \} \textbf{in} $e$ transforms to \textbf{let} \{ $d_1$  \} \textbf{in} \textbf{let} \{ $d_2$ \} \textbf{in} $e$ when no identifier bound in $d_2$ appears free in $d_1$.
\end{enumerate}

\subsubsection{Generalization} \label{generalization}

The Hindley-Milner type system assigns types to a \texttt{let}-expression in two stages.
First, the right-hand sides of the declarations are typed, giving types with no universal quantification.
Second, all type variables that occur in these types are universally quantified unless they are associated with bound variables in the type environment; this is called generalization.
Finally, the body of the \texttt{let}-expression is typed.

For example, consider the declaration \exq{
f = let g y = (y,y) in (g true, g 0)
}
The type of \texttt{g}'s right-hand side is $a \rightarrow{} (a,a)$. The generalization step attributes to \texttt{g} the polymorphic type $\forall a. a \rightarrow{} (a,a)$, after which the typing of the body part can proceed. It is precisely the generalization step that makes \texttt{g} independent of any other types and allows its usage in the body at \emph{different} types.

\frege{} currently imposes an extra restriction on the generalization step: only function types can be generalized. This restriction is needed in the presence of modifiable data values to guarantee type safety. Without this restriction, the following would be possible:
\begin{code}
    x = let
        a = Array.new 1
      in case a.[0 <- Just true] of
            () | Just i <- a.[0] = i + 42
\end{code}
The array would get a type of $\forall a$.Array $a$ which means something like "whatever data one stores in the array, array element access will always return a value of the right type", which is of course impossible.

Sometimes it is not possible to generalize over all the type variables used in the type of the definition. For example, consider the declaration  \exq{
f x = let g z y = ([x,z],y) in ...
}
In an environment where \texttt{x} has type $a$, the type of \texttt{g}'s defining expression is $a \rightarrow{} b \rightarrow{} ([a],b)$. The generalization step attributes to \texttt{g} the type $\forall{} b$.$a \rightarrow{} b \rightarrow{} ([a],b)$; only $b$ can be universally quantified because $a$ occurs in the type environment. We say that the type of \texttt{g} is \emph{monomorphic in the type variable a}.

One can put this differently by saying that $a$ is a type variable that  can still be unified with an actual type (see below), while for a quantified type variable like $b$ the type checker has found that the actual type (of the variable \texttt{y}, in our case) can be any type within the definition and neither has any influence on types outside the definition nor is it influenced by other types.

The effect of such monomorphism is that the first argument of all applications of \texttt{g} must be of a single type. For example, it would be valid for the "..." to be \exq{
(g true 0, g false 1)}
(which would, incidentally, force \texttt{x} to have type \texttt{Bool}) but invalid for it to be \exq{
(g true 0, g 'c' 1)}

It is worth noting that the explicit type signatures provided by \frege{} are not powerful enough to express types that include monomorphic type variables.
For example, we cannot write
\begin{code}
    f x = let
            g :: a -> b -> ([a],b)
            g y z = ([x,y], z)
        in ...
\end{code}
because that would claim that \texttt{g} was polymorphic in both \texttt{a} and \texttt{b}. In this program, \texttt{g} can only be given a type signature if its first argument is restricted to a type not involving type variables; for example \exq{
g :: Int -> b -> ([Int],b)}
This signature would also cause \texttt{x} to have type \texttt{Int}.

\subsubsection{Higher Rank Polymorphism} \label{higher-rank}

In the Hindley-Milner type system, the types of lambda bound variables are always monomorphic. This restriction keeps type inference decidable, but excludes \emph{higher rank polymorphism}, that is, the ability to write functions that take polymorphic functions as arguments. For an in depth discussion of these matters see \cite{ptifart}. The conservative extension of the type system proposed in that paper is also implemented in the \frege{} type system.

To exemplify the problem, consider the following program:

\begin{code}
foo :: ([Bool], [Char])
foo = let
        f x = (x [true,false], x ['a','b','c'])
    in f reverse
\end{code}

In the body of \texttt{f}, the function \texttt{x} is applied both to a list of booleans and a list of characters - but that should be fine because the function passed to \texttt{f}, namely \texttt{reverse} (a library function with type $\forall{} a$.$[a] \rightarrow{} [a]$) works equally well on lists of any type. Nevertheless, the expression is rejected as it stands. With the restriction on lambda bound arguments, the type checker can assign to \texttt{x} the type \texttt{[Bool] -> [Bool]} or \texttt{[Char] -> [Char]} but not $\forall{} a$.$[a] \rightarrow{} [a]$.

The \frege{} type checker lifts this restriction by implemeting the following rules:
\begin{itemize}
\item In type annotated patterns of the form $(p$\sym{::}$t)$, where $t$ is not a function type, the type variables in $t$ will not be universally quantified, that is, the type variables are regarded as monomorphic.
\item However, if $t$ is a function type, the type variables in $t$ will be universally quantified.
\item If the pattern is not annotated, the normal rules are in effect, that is, the type system will infer a monomorphic type.
\end{itemize}
These rules allow to have a function argument that is a polymorphic function, but this must be explicitely required by annotating corresponding the pattern accordingly.
Thus, the example above can be rewritten in \frege{}

\begin{code}
foo :: ([Bool], [Char])
foo = let
        // f will take a polymorphic function as argument
        f (x::[a]->[a]) = (x [true,false], x ['a','b','c'])
    in f reverse
\end{code}

This causes the type checker to infer the following type for \texttt{f}: \exq{f :: ($\forall{} a$.$[a] \rightarrow{} [a]$) -> ([Bool], [Char])}

Note that it is not possible to write this type directly due to lack of syntax for explicit quantification. Also, since the type annotations on patterns can not themselves express higher rank function types, higher rank polymorphism is currently restricted to rank 2.

\hasdiff{Haskell 98 does not allow higher rank polymporphism, while extensions like GHC do.}

\subsection{Native Declaration} \label{nativefun}
\index{declaration!native function}

\begin{flushleft}
\rul{nativefun} \term{native} \nont{annoitem} \opt{\nont{javaitem}} \sym{::} \nont{type}\\
\rul{javaitem} \nont{nativename} \oder{} \nont{unop} \oder{} \nont{symop} \oder{} \nont{stringliteral}
\end{flushleft}

The general form of the \emph{native declaration}
\begin{quote}
\textbf{native} $v$  $j$ \textbf{::} $t$
\end{quote}
introduces a new variable $v$ with type $t$, that will behave like any other \frege{} variale of that type but is implemented in \java{}. $j$ is a string value that contains information regarding $v$'s \java{} implementation. For convenience, $j$ can be written without quotes as long as the names or operators specified would also be valid in \frege{}. Often, it is the case that $v$ and $j$ are the same, as in \exq{
    data PrintStream = native java.io.PrintStream\\
    native println println :: PrintStream -> ()
}
In such cases, $j$ can simply be omitted.

The two declarations above introduce a data type \texttt{PrintStream} that is based on the \java{} class \texttt{java.io.PrintStream} and a function \texttt{println} that takes arguments of type \texttt{PrintStream} but returns nothing useful. By the rules of the \hyperref[native interface]{native interface}, the implementation of this function is actually the \texttt{println} method of the \texttt{java.io.PrintStream} class.

There must be no binding for $v$ in the same scope, since this would be a contradiction to the native declaration that essentially says that $v$ is implemented in \java{}, not in \frege{}.

The compiler cannot check the validity of the native declaration, only very basic sanity checks are possible. Errors not detectable (for example incorrect type information, spelling errors in identifiers, etc.) will normally cause failures during compilation of the genrated \java{} code. The presence of invalid native declarations should be the only reason for rejection of the generated code by the \java{} compiler\footnote{Another reason would be bugs in the \java{} or \frege{} compilers}; thus whenever \texttt{javac} complains, one should first check the native declarations.

During code generation, expressions that contain $v$ are mapped to certain \java{} consructs such as

\begin{itemize}
\item field access expressions (see \cite[section 15.11]{langspec3}) for static fields
\item method invocation expressions (see \cite[section 15.12]{langspec3}) for both static and non-static methods
\item class instance creation expressions (see \cite[section 15.9]{langspec3})
\item unary expressions (see \cite[section 15.15]{langspec3})
\item cast expressions (see \cite[section 15.16]{langspec3})
\item binary expressions (see \cite[section 15.17 to 15.24]{langspec3})
\end{itemize}

in a transparent and well defined way. This is explained in detail in \autoref{native interface}.
